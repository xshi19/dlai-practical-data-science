{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a text classifier using Amazon SageMaker BlazingText built-in algorithm\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this lab you will use SageMaker BlazingText built-in algorithm to predict the sentiment for each customer review. BlazingText is a variant of FastText which is based on word2vec. For more information on BlazingText, see the documentation here:  https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [1. Prepare dataset](#c1w4-1.)\n",
    "  - [1.1. Load the dataset](#c1w4-1.1.)\n",
    "  - [1.2. Transform the dataset](#c1w4-1.2.)\n",
    "    - [Exercise 1](#c1w4-ex-1)\n",
    "  - [1.3. Split the dataset into train and validation sets](#c1w4-1.3.)\n",
    "  - [1.4. Upload the `train` and `validation` datasets to S3 bucket](#c1w4-1.4.)\n",
    "- [2. Train the model](#c1w4-2.)\n",
    "  - [Exercise 2](#c1w4-ex-2)\n",
    "  - [Exercise 3](#c1w4-ex-3)\n",
    "  - [Exercise 4](#c1w4-ex-4)\n",
    "  - [Exercise 5](#c1w4-ex-5)\n",
    "  - [Exercise 6](#c1w4-ex-6)\n",
    "  - [Exercise 7](#c1w4-ex-7)\n",
    "- [3. Deploy the model](#c1w4-3.)\n",
    "- [4. Test the model](#c1w4-4.)\n",
    "\n",
    "Let's install and import required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# please ignore warning messages during the installation\n",
    "!pip install --disable-pip-version-check -q sagemaker==2.35.0\n",
    "!pip install --disable-pip-version-check -q nltk==3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c1w4-1.'></a>\n",
    "# 1. Prepare dataset\n",
    "Let's adapt the dataset into a format that BlazingText understands. The BlazingText format is as follows:\n",
    "\n",
    "```\n",
    "__label__<label> \"<features>\"\n",
    "```\n",
    "\n",
    "Here are some examples:\n",
    "```\n",
    "__label__-1 \"this is bad\"\n",
    "__label__0 \"this is ok\"\n",
    "__label__1 \"this is great\"\n",
    "```\n",
    "\n",
    "Sentiment is one of three classes: negative (-1), neutral (0), or positive (1).  BlazingText requires that `__label__` is prepended to each sentiment value.\n",
    "\n",
    "You will tokenize the `review_body` with the Natural Language Toolkit (`nltk`) for the model training. `nltk` documentation can be found [here](https://www.nltk.org/). You will also use `nltk` later in this lab to tokenize reviews to use as inputs to the deployed model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c1w4-1.1.'></a>\n",
    "### 1.1. Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the dataset into the Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp 's3://dlai-practical-data-science/data/balanced/womens_clothing_ecommerce_reviews_balanced.csv' ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './womens_clothing_ecommerce_reviews_balanced.csv'\n",
    "\n",
    "df = pd.read_csv(path, delimiter=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c1w4-1.2.'></a>\n",
    "### 1.2. Transform the dataset\n",
    "Now you will prepend `__label__` to each sentiment value and tokenize the review body using `nltk` module. Let's import the module and download the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split a sentence into tokens you can use `word_tokenize` method. It will separate words, punctuation, and apply some stemming. Have a look at the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I'm not a fan of this product!\"\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of word tokenization can be converted into a string separated by spaces and saved in the dataframe. The transformed sentences are prepared then for better text understending by the model. \n",
    "\n",
    "Let's define a `prepare_data` function which you will apply later to transform both training and validation datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c1w4-ex-1'></a>\n",
    "### Exercise 1\n",
    "\n",
    "Apply the tokenizer to each of the reviews in the `review_body` column of the dataframe `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(review):\n",
    "    # delete commas and quotation marks, apply tokenization and join back into a string separating by spaces\n",
    "    return ' '.join([str(token) for token in nltk.word_tokenize(str(review).replace(',', '').replace('\"', '').lower())])\n",
    "    \n",
    "def prepare_data(df):\n",
    "    df['sentiment'] = df['sentiment'].map(lambda sentiment : '__label__{}'.format(str(sentiment).replace('__label__', '')))\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    df[None] = df[None].map(lambda review : tokenize(review)) # Replace all None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the prepared function and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sample dataframe\n",
    "df_example = pd.DataFrame({\n",
    "    'sentiment':[-1, 0, 1], \n",
    "    'review_body':[\n",
    "        \"I do like this product!\", \n",
    "        \"this product is ok\", \n",
    "        \"I don't like this product!\"]\n",
    "})\n",
    "\n",
    "# test the prepare_data function\n",
    "print(prepare_data(df_example))\n",
    "\n",
    "# Expected output:\n",
    "#      sentiment                   review_body\n",
    "# 0  __label__-1      i do like this product !\n",
    "# 1   __label__0            this product is ok\n",
    "# 2   __label__1  i do n't like this product !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the `prepare_data` function to the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blazingtext = df[['sentiment', 'review_body']].reset_index(drop=True)\n",
    "df_blazingtext = prepare_data(df_blazingtext)\n",
    "df_blazingtext.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c1w4-1.3.'></a>\n",
    "### 1.3. Split the dataset into train and validation sets\n",
    "Split and visualize a pie chart of the train (90%) and validation (10%) sets. You can do the split using the `sklearn` model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split all data into 90% train and 10% holdout\n",
    "df_train, df_validation = train_test_split(df_blazingtext, \n",
    "                                           test_size=0.10,\n",
    "                                           stratify=df_blazingtext['sentiment'])\n",
    "\n",
    "labels = ['train', 'validation']\n",
    "sizes = [len(df_train.index), len(df_validation.index)]\n",
    "explode = (0.1, 0)  \n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "\n",
    "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "ax1.axis('equal')  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results as CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blazingtext_train_path = './train.csv'\n",
    "df_train[['sentiment', 'review_body']].to_csv(blazingtext_train_path, index=False, header=False, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blazingtext_validation_path = './validation.csv'\n",
    "df_validation[['sentiment', 'review_body']].to_csv(blazingtext_validation_path, index=False, header=False, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c1w4-1.4.'></a>\n",
    "### 1.4. Upload the `train` and `validation` datasets to S3 bucket\n",
    "You will use these to train and validate your model. Let's save them to S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s3_uri = sess.upload_data(bucket=bucket, key_prefix='blazingtext/data', path=blazingtext_train_path)\n",
    "validation_s3_uri = sess.upload_data(bucket=bucket, key_prefix='blazingtext/data', path=blazingtext_validation_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c1w4-2.'></a>\n",
    "# 2. Train the model\n",
    "\n",
    "Setup the BlazingText estimator. For more information on Estimators, see the SageMaker Python SDK documentation here: https://sagemaker.readthedocs.io/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c1w4-ex-2'></a>\n",
    "### Exercise 2\n",
    "\n",
    "Setup the container image to use for training with the BlazingText algorithm.\n",
    "\n",
    "**Instructions**: Use the `sagemaker.image_uris.retrieve` function with the `blazingtext` algorithm. \n",
    "\n",
    "```python\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    region=region,\n",
    "    framework='...' # the name of framework or algorithm\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    region=region,\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    framework=None # Replace None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c1w4-ex-3'></a>\n",
    "### Exercise 3\n",
    "\n",
    "Create an estimator instance passing the container image and other instance parameters.\n",
    "\n",
    "**Instructions**: Pass the container image prepared above into the `sagemaker.estimator.Estimator` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    image_uri=None, # Replace None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    role=role, \n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.large',\n",
    "    volume_size=30,\n",
    "    max_run=7200,\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the hyper-parameters for BlazingText. You are using BlazingText for a supervised classification task. For more information on the hyper-parameters, see the documentation here:  https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext-tuning.html\n",
    "\n",
    "The hyperparameters that have the greatest impact on word2vec objective metrics are: `learning_rate` and `vector_dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(mode='supervised',   # supervised (text classification)\n",
    "                              epochs=10,           # number of complete passes through the dataset: 5 - 15\n",
    "                              learning_rate=0.01,  # step size for the  numerical optimizer: 0.005 - 0.01\n",
    "                              min_count=2,         # discard words that appear less than this number: 0 - 100                              \n",
    "                              vector_dim=300,      # number of dimensions in vector space: 32-300\n",
    "                              word_ngrams=3)       # number of words in a word n-gram: 1 - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To call the `fit` method for the created estimator instance you need to setup the input data channels. This can be organized as a dictionary\n",
    "\n",
    "```python\n",
    "data_channels = {\n",
    "    'train': ..., # training data\n",
    "    'validation': ... # validation data\n",
    "}\n",
    "```\n",
    "\n",
    "where training and validation data are the Amazon SageMaker channels for S3 input data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c1w4-ex-4'></a>\n",
    "### Exercise 4\n",
    "\n",
    "Create a train data channel.\n",
    "\n",
    "**Instructions**: Pass the S3 input path for training data into the `sagemaker.inputs.TrainingInput` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.inputs.TrainingInput(\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    None, # Replace None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    distribution='FullyReplicated', \n",
    "    content_type='text/plain', \n",
    "    s3_data_type='S3Prefix'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c1w4-ex-5'></a>\n",
    "### Exercise 5\n",
    "\n",
    "Create a validation data channel.\n",
    "\n",
    "**Instructions**: Pass the S3 input path for validation data into the `sagemaker.inputs.TrainingInput` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = sagemaker.inputs.TrainingInput(\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    None, # Replace None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    distribution='FullyReplicated', \n",
    "    content_type='text/plain', \n",
    "    s3_data_type='S3Prefix'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c1w4-ex-6'></a>\n",
    "### Exercise 6\n",
    "\n",
    "Organize the data channels defined above as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_channels = {\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    'train': None, # Replace None\n",
    "    'validation': None # Replace None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c1w4-ex-7'></a>\n",
    "### Exercise 7\n",
    "\n",
    "Start fitting the model to the dataset.\n",
    "\n",
    "**Instructions**: Call the `fit` method of the estimator passing the configured train and validation inputs (data channels).\n",
    "\n",
    "```python\n",
    "estimator.fit(\n",
    "    inputs=..., # train and validation input\n",
    "    wait=False # do not wait for the job to complete before continuing\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    inputs=None, # Replace None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    wait=False\n",
    ")\n",
    "\n",
    "training_job_name = estimator.latest_training_job.name\n",
    "print('Training Job Name:  {}'.format(training_job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the training job in the console.\n",
    "\n",
    "**Instructions**: \n",
    "- open the link\n",
    "- notice that you are in the section `Amazon SageMaker` -> `Training jobs`\n",
    "- check the name of the training job, its status and other available information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/jobs/{}\">Training job</a></b>'.format(region, training_job_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the Cloud Watch logs (after about 5 minutes).\n",
    "\n",
    "**Instructions**: \n",
    "- open the link\n",
    "- open the log stream with the name, which starts from the training job name\n",
    "- have a quick look at the log messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/TrainingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch logs</a> (after about 5 minutes)</b>'.format(region, training_job_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the training job to complete.\n",
    "\n",
    "### _This cell will take approximately 5-10 minutes to run._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "estimator.latest_training_job.wait(logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the train and validation accuracy.\n",
    "\n",
    "_Ignore any warnings._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.training_job_analytics.dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the trained model in the S3 bucket.\n",
    "\n",
    "**Instructions**: \n",
    "- open the link\n",
    "- notice that you are in the section `Amazon S3` -> `[bucket name]` -> `[training job name]` (Example: `Amazon S3` -> `sagemaker-us-east-1-82XXXXXXXXXXX` -> `blazingtext-20XX-XX-XX-XX-XX-XX-XXX`)\n",
    "- check the existence of the `model.tar.gz` file in the `output` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/output/?region={}&tab=overview\">Trained model</a> in S3</b>'.format(bucket, training_job_name, region)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c1w4-3.'></a>\n",
    "# 3. Deploy the model\n",
    "\n",
    "Now deploy the trained model as an Endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _This cell will take approximately 5-10 minutes to run._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "text_classifier = estimator.deploy(initial_instance_count=1,\n",
    "                                   instance_type='ml.m5.large',\n",
    "                                   serializer=sagemaker.serializers.JSONSerializer(),\n",
    "                                   deserializer=sagemaker.deserializers.JSONDeserializer())\n",
    "\n",
    "print()\n",
    "print('Endpoint name:  {}'.format(text_classifier.endpoint_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the endpoint in the AWS console.\n",
    "\n",
    "**Instructions**: \n",
    "- open the link\n",
    "- notice that you are in the section `Amazon SageMaker` -> `Endpoints` -> `[Endpoint name]` (Example: `Amazon SageMaker` -> `Endpoints` -> `blazingtext-20XX-XX-XX-XX-XX-XX-XXX`)\n",
    "- check the status and other available information about the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/endpoints/{}\">SageMaker REST Endpoint</a></b>'.format(region, text_classifier.endpoint_name)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='c1w4-4.'></a>\n",
    "# 4. Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the `nltk` library to convert the raw reviews into tokens that BlazingText recognizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify sample reviews to predict the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = ['This product is great!',\n",
    "           'OK, but not great',\n",
    "           'This is not the right product.'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the reviews and specify the payload to use when calling the REST API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_reviews = [' '.join(nltk.word_tokenize(review)) for review in reviews]\n",
    "\n",
    "payload = {\"instances\" : tokenized_reviews}\n",
    "print(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can predict the sentiment for each review. Call the `predict` method of the text classifier passing the tokenized sentence instances (`payload`) into the data argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = text_classifier.predict(data=payload)\n",
    "for prediction in predictions:\n",
    "    print('Predicted class: {}'.format(prediction['label'][0].lstrip('__label__')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the notebook into S3 bucket for grading purposes.\n",
    "\n",
    "**Note**: you may need to click on \"Save\" button before the upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp ./C1_W4_Assignment.ipynb s3://$bucket/C1_W4_Assignment_Learner.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please go to the main lab window and click on `Submit` button (see the `Finish the lab` section of the instructions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
